{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1 - Data Exploration & Cleaning, EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### 1.1 Load the Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta \n",
    "# datetime objects represent specific points in time\n",
    "# timedelta objects represent durations.    \n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "start_date = datetime(2020, 1, 1)\n",
    "num_months = 48 # 4 years of data (business cycle)\n",
    "\n",
    "date_rng = pd.date_range(start_date, periods = num_months, freq = 'MS') \n",
    "# Creates a sequence of monthly timestamps, starting at 2020-01-01, with 48 entries, each aligned to the start of the month ('MS' = Month Start).\n",
    "# MS = Month Start. It means the dates will be aligned to the start of each month (e.g., Jan 1, Feb 1, Mar 1, etc.).\n",
    "\n",
    "sales_data = pd.DataFrame(date_rng, columns = ['Date'])\n",
    "\n",
    "\n",
    "# Base sales with a slight upward trend\n",
    "base_sales = 10000 + np.arange(num_months) * 50 # create a NumPy array named base_sales\n",
    "# This line is generating a sequence of increasing monthly base sales, starting from 10,000 and increasing by 50 units per month\n",
    "# np.arange() is a NumPy function that returns an array of evenly spaced values within a specified range.\n",
    "# Starts the sales at 10,000 units in the first month (January 2020), and increases by 50 each subsequent month.\n",
    "# sales grow over time due to various factors (e.g., market expansion, product maturity).\n",
    "\n",
    "\n",
    "# We're creating a seasonal pattern — like in sales, temperature, or web traffic — that repeats predictably throughout the year\n",
    "seasonal_component = 2000 * np.sin(2 * np.pi * (sales_data['Date'].dt.month - 1) / 12 + np.pi/2) \\\n",
    "                   + 1500 * np.sin(2 * np.pi * (sales_data['Date'].dt.month - 1) / 6)\n",
    "# This results in a seasonal pattern like:\n",
    "    #High: Oct–Dec\n",
    "    #Low: Jan–Feb\n",
    "    #Smaller bumps: May–Jun, Aug–Sep\n",
    "\n",
    "# The number 2000 just means: “Make this wave big enough to matter in sales.”\n",
    "\n",
    "\n",
    "# Generating random fluctuations — like tiny unpredictable changes in sales (e.g., weather, supply hiccups, customer behavior).\n",
    "noise = np.random.normal(0, 500, num_months)  \n",
    "# This uses a normal distribution (bell curve) with:\n",
    "    # Mean = 0 → centered around zero (so noise can go positive or negative)\n",
    "    # Standard deviation = 500 → most noise will fall between -500 and +500\n",
    "    # num_months = how many months of noise you want\n",
    "\n",
    "sales_data['SalesAmount'] = (base_sales + seasonal_component + noise).astype(int)\n",
    "sales_data['SalesAmount'] = sales_data['SalesAmount'].clip(lower=2000) #Ensure no negative sales\n",
    "\n",
    "\n",
    "\n",
    "# Add promotional flags (randomly)\n",
    "sales_data['Promotion'] = np.random.choice([0,1], size = num_months, p = [0.8, 0.2])\n",
    "\n",
    "# You randomly mark whether there was a promotion each month:\n",
    "    # this generates 48 random choices\n",
    "    # 80% chance of picking 0 (no promotion)\n",
    "    # 20% chance of picking 1 (promotion active)\n",
    "    # So in 48 months:\n",
    "        # Around 38 months will have no promotion\n",
    "        # Around 10 months will have a promotion\n",
    "    # This simulates marketing events (like discounts, campaigns) that happen occasionally.\n",
    "\n",
    "\n",
    "# Increase sales during promotions\n",
    "sales_data.loc[sales_data['Promotion'] == 1, 'SalesAmount'] *= np.random.uniform(1.1, 1.3, size = (sales_data['Promotion'] == 1).sum())\n",
    "\n",
    "# If a month had a promotion, it increases sales by 10% to 30%, using a random multiplier between 1.1 and 1.3.\n",
    "\n",
    "\n",
    "# Add Holiday Flags\n",
    "\n",
    "sales_data[\"HolidayMonth\"] = (sales_data['Date'].dt.month == 12).astype(int)  # December is a holiday month\n",
    "#sales_data.loc[sales_data[\"HolidayMonth\"] == 1, \"SalesAmount\"] *= np.random.uniform(1.15, 1.4) # Increase sales in December by 15% to 40% during holidays\n",
    "sales_data.loc[sales_data[\"HolidayMonth\"] == 1, \"SalesAmount\"] *= np.random.uniform(1.15, 1.4, size=(sales_data[\"HolidayMonth\"] == 1).sum())\n",
    "\n",
    "\n",
    "sales_data[\"SalesAmount\"] = sales_data[\"SalesAmount\"].astype(int)\n",
    "\n",
    "sales_data.to_csv(\"retail_sales_mock_data.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### 1.2 Convert date columns to datetime objects and set as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sales_data = pd.read_csv(\"retail_sales_mock_data.csv\")\n",
    "\n",
    "sales_data['Date'] = pd.to_datetime(sales_data['Date'])\n",
    "\n",
    "sales_data.set_index(\"Date\", inplace=True)\n",
    "sales_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 1.3 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Visualize missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.bar(sales_data)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Visualize Sales over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your job is to convert sales history into actionable insight:\n",
    "# “Here’s when to increase stock.”\n",
    "# “Here’s when to launch campaigns.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.plot(sales_data.index, sales_data['SalesAmount'])\n",
    "\n",
    "plt.title(\"Sales Amount Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales Amount\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Check for trends, seasonality, and stationarity (e.g., using decomposition plots, ACF/PACF plots, Dickey-Fuller test)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Check Trends through Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "##### (a) Classical Decomposition Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separates data into\n",
    "    # Trend:A long-term increase or decrease in the data over time\n",
    "    # Seasonality: A pattern that repeats at regular intervals (e.g., weekly, monthly, yearly).\n",
    "    # Stationarity: A property of a time series where statistical properties (like mean and variance) remain constant over time.\n",
    "        # tells you: “Can I rely on past data to predict the future, or is the game changing?”\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'SalesAmount' is a float type (required by statsmodels)\n",
    "sales_data['SalesAmount'] = sales_data['SalesAmount'].astype(float)\n",
    "\n",
    "# Decompose using additive model (common for sales data)\n",
    "decomposition = seasonal_decompose(sales_data['SalesAmount'], model='additive', period=12)\n",
    "\n",
    "# Plot the decomposition\n",
    "decomposition.plot()\n",
    "\n",
    "plt.suptitle('Time Series Decomposition of Sales Data', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "##### (b) STL (Seasonal and Trend decomposition using Loess) Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# STL expects float values\n",
    "sales_data['SalesAmount'] = sales_data['SalesAmount'].astype(float)\n",
    "\n",
    "# STL decomposition with period=12 for yearly seasonality\n",
    "stl = STL(sales_data['SalesAmount'], period=12, robust=True)\n",
    "result = stl.fit()\n",
    "\n",
    "# Plotting\n",
    "fig = result.plot()\n",
    "plt.suptitle(\"STL Decomposition of Sales Data\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Check Seasonality through ACF and PACF Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### ACF: AutoCorrelation Function Plot: Shows total correlation with past values\n",
    "#### PACF: Partial AutoCorrelation Function: Shows direct correlation with past values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation is a way to measure how two things (numbers or variables) move together.\n",
    "# Correlation is between two different things\n",
    "# Correlation is used to understand relationships between variables (e.g., ads vs. sales).\n",
    "# A correlation of +1 means a perfect positive relationship, -1 means a perfect negative relationship, and 0 means no linear relationship\n",
    "\n",
    "\n",
    "# Autocorrelation is between the same thing at different times\n",
    "# Autocorrelation is used to understand patterns in time (e.g., seasonal trends or repeated behaviors).\n",
    "\n",
    "# ACF: Shows how current values are related to previous values\n",
    "       # Helps detect repeating patterns, trends, or cycles.\n",
    "       # Used to figure out the MA (Moving Average) part of forecasting models.\n",
    "\n",
    "#PACF: Similar to ACF, but it removes the influence of the values in between.\n",
    "       # Helps find the true relationship between the current value and a past value, without the “middlemen”.\n",
    "       # Used to figure out the AR (Autoregressive) part of forecasting models.\n",
    "\n",
    "# When building models like ARIMA:\n",
    "       # PACF helps decide how many past values (lags) to use → this is AR part.\n",
    "       # ACF helps decide how many past errors to use → this is MA part.\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#ACF plot\n",
    "plot_acf(sales_data[\"SalesAmount\"], lags=24) #  We are comparing the SalesAmount column to itself at different time shifts (lags).\n",
    "# with lags=24, you're checking how well current sales correlate with sales from each of the past 24 months, one lag at a time.\n",
    "plt.title(\"Autocorrelation Function (ACF) of Sales Amount\")\n",
    "plt.xlabel(\"Lags\")\n",
    "plt.ylabel(\"ACF\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#PACF Plot\n",
    "plot_pacf(sales_data[\"SalesAmount\"], lags=24)\n",
    "plt.title(\"Partial Autocorrelation Function (PACF) of Sales Amount\")\n",
    "plt.xlabel(\"Lags\")\n",
    "plt.ylabel(\"PACF\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Check Stationarity through Dickey-Fuller test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### The Augmented Dickey-Fuller (ADF) test checks if a unit root is present in the series, which would mean the series is non-stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p > 0.05 → Non-stationary;  Apply transformations: differencing, log, seasonal adjustment\n",
    "# p <= 0.05 → Stationary; No transformations needed. Proceed with ARIMA, SARIMA, or other time series models\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "result = adfuller(sales_data['SalesAmount'])\n",
    "print(\"ADF Statistic:\", result[0])\n",
    "print(\"p-value:\", result[1])\n",
    "print(\"Number of lags used: \", result[2])\n",
    "print(\"Number of observations used for ADF regression and critical values calculation:\", result[3])\n",
    "print(\"Critical Values:\")\n",
    "for key, value in result[4].items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "#    METRIC\t        VALUE\t                               INTERPRETATION\n",
    "# ADF Statistic  \t-4.333890\t                 Very negative → strong evidence against unit root\n",
    "# p-value\t        0.00038\t                   < 0.05 → Reject the null hypothesis (H₀)\n",
    "# Conclusion\tThe data is stationary\t          You don’t need differencing before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data[\"SalesAmount\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# 2 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 2.1 Create Lag Features (A lag feature is the value of the target variable from a previous time step.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### Function that creates lag features (reusable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_lag_features(df, target_column: str, lags: list, drop_na: bool = True) -> pd.DataFrame: #type hints have been used\n",
    "    # Now I am starting a docstring: a description of what the function does, how to use it, and what it returns\n",
    "    \"\"\"\n",
    "    Adds lag features to the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame (must be time-ordered with a datetime index).\n",
    "    target_column : str\n",
    "        The column name to create lags for (e.g., 'SalesAmount').\n",
    "    lags : list\n",
    "        A list of integer lag periods (e.g., [1, 2, 3, 6, 12]).\n",
    "    drop_na : bool\n",
    "        If True, drops rows with NaN introduced by lagging.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with new lag columns added.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()  \n",
    "    # non-destructive programmimg; Makes a copy of the original data table so we don’t accidentally change the original data. \n",
    "\n",
    "    for x in lags:\n",
    "        lag_col = f'{target_column}_lag_{x}'\n",
    "        df[lag_col] = df[target_column].shift(x)\n",
    "        # pandas.shift() moves the values in a Series or DataFrame up or down by a specified number of periods while keeping the original index unchanged.\n",
    "\n",
    "    if drop_na:\n",
    "        df.dropna(inplace=True) #This is a built-in Pandas function used to remove rows (or columns) with missing values (NaN) from a DataFrame.\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create_lag_features(df, target_column: str, lags: list, drop_na: bool = True)\n",
    "\n",
    "lags_to_create = [1,2,3,6,12]\n",
    "\n",
    "\"\"\"\n",
    "1, 2, 3: Sales from the previous 1, 2, and 3 days.\n",
    "\n",
    "6: Sales from 6 days ago (useful for weekly patterns, as it's close to a week).\n",
    "\n",
    "12: Sales from 12 days ago (often used to capture bi-weekly effects or just a longer history).\n",
    "\"\"\"\n",
    "\n",
    "sales_data_with_lags = create_lag_features(df = sales_data, target_column= 'SalesAmount', lags = lags_to_create, drop_na = True)\n",
    "\n",
    "sales_data_with_lags.to_csv(\"retail_sales_with_lags.csv\", index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data_with_lags.info()\n",
    "\n",
    "# The original data had 48 months of sales.\n",
    "# But when I create lag features — like \"What were the sales 12 months ago?\" — I can't fill in that info for the first 12 months, because there's no earlier data to look back on. \n",
    "# So those early rows end up with missing values and get removed.\n",
    "# That's why the new table has only 36 months of complete data, starting from the 13th month (January 2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data_with_lags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Check Stationarity of this new data with lags using Dickey Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "result = adfuller(sales_data_with_lags['SalesAmount'])\n",
    "print(\"ADF Statistic:\", result[0])\n",
    "print(\"p-value:\", result[1])\n",
    "print(\"Number of lags used: \", result[2])\n",
    "print(\"Number of observations used for ADF regression and critical values calculation:\", result[3])\n",
    "print(\"Critical Values:\")\n",
    "for key, value in result[4].items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Data is still stationary after adding lag features\n",
    "# Now we can use the lagged features to build predictive models, like ARIMA or SARIMA, to forecast future sales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "#### ACF and PACF Plots again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "#### ACF (Memory of Past Errors) - Moving Average (MA)\n",
    "####  PACF (Memory of Past Sales) - Auto Regressive (AR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_acf(sales_data_with_lags[\"SalesAmount\"], lags = 17)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plot_pacf(sales_data_with_lags[\"SalesAmount\"], lags = 17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "# 3.\tModel Selection and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### 3.1 •\tSplit data into training and validation sets (respecting temporal order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = sales_data_with_lags.iloc[:-6] # First 30 months for training (Jan 2021-Jun 2023)\n",
    "validationSet = sales_data_with_lags.iloc[-6:] # Last 6 months for validation (Jul 2023 - Dec 2023)\n",
    "\n",
    "print(\"Training Set: \" , trainingSet.shape)\n",
    "print(\"Validation Set: \", validationSet.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### 3.2 Implement ARIMA (Autoregressive Integrated Moving Average) 1,0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "arima_model = ARIMA(trainingSet['SalesAmount'], order = (1,0,1))\n",
    "\n",
    "# AR (p) = 1 → use sales from 1 month ago / Use the last month’s sales\n",
    "# I (d) = 0 → don’t difference (because data is stationary)\n",
    "# MA (q) = 1 → use noise from 1 month ago / Use the last month's error in prediction\n",
    "\n",
    "\n",
    "arima_result = arima_model.fit()\n",
    "# This is where the machine “learns” the best-fit values for the AR and MA components.\n",
    "# It finds parameters that minimize the error (difference between predicted vs. actual sales) on the training data.\n",
    "\n",
    "print(arima_result.summary())\n",
    "\n",
    "# log likelihood (-261.472) = The closer to zero (less negative), the better.\n",
    "    # -261.472 is not terrible, but it’s not excellent either.\n",
    "    #  It tells us the model does an “okay” job at explaining the data — not super tight, but not wildly off.\n",
    "\n",
    "\n",
    "# AIC (Akaike Information Criterion (530.945) = Lower is better. \n",
    "    # AIC tries to balance two goals:\n",
    "        # Accuracy (does the model fit the data well?)\n",
    "        # Simplicity (is it unnecessarily complicated?)\n",
    "    # AIC = 530.945 is not meaningful on its own. It only means something when you compare it to other models on the same dataset:\n",
    "\n",
    "\n",
    "# BIC (Bayesian Information Criterion (536.549) = Lower is better. Similar to AIC but penalizes complexity more.\n",
    "\n",
    "# HQIC (Hannan-Quinn Information Criterion (532.738) = Lower is better. Another model selection criterion.\n",
    "\n",
    "\n",
    "\n",
    "# values (AIC 530, BIC 536, HQIC 532) are all moderate — not bad, but you won’t know if they’re good until you try another model to compare.\n",
    "\n",
    "\n",
    "# ar.L1 = 0.5054, p = 0.074: \"How much does last month’s sales affect this month?\" / Past sales’ impact\n",
    "      # 0.5054 → moderately positive relationship\n",
    "             # → If last month was high, this month tends to be high too.\n",
    "      # AR(1) has some predictive strength (p ≈ 0.07) — it’s likely contributing to short-term trends.\n",
    "\n",
    "# ma.L1 = 0.4049, p = 0.160: \"How much does last month’s prediction error affect this month?\" / Past error’s impact\n",
    "        # 0.4049 → moderate positive relationship\n",
    "                 # → If last month’s prediction was off, this month tends to follow that pattern.\n",
    "        # p = 0.160 → Not significant at all (you typically want < 0.05)\n",
    "# This MA term is probably not helping much. You could try removing it (i.e., test ARIMA(1,0,0)) and see if performance holds.\n",
    "\n",
    "# p-value for AR/MA terms (0.074, 0.160)\tNot < 0.05 → ⚠️\tSuggests these coefficients may not help much\n",
    "\n",
    "\n",
    "# σ² (sigma squared) = 2,116,000\n",
    "# This is the variance of the error term. It tells you: “How far off is the model, on average, from the actual sales?”\n",
    "# This value being large (~2 million) means your model's predictions can still be off by a big margi\n",
    "# This tells us: there's still a lot of variance in actual sales not explained by your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "##### This ARIMA(1,0,1) model provides a moderate fit to your retail sales data — it captures some short-term dependencies (like last month's sales) but doesn't fully explain all variation, as indicated by high residual variance. The autoregressive term (AR) is borderline significant, while the moving average term (MA) likely adds little value, suggesting a simpler model might work just as well or better. Overall, the model is functional but not optimal, and should be refined or compared against alternatives for better forecasting reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "#### Forecast on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast next 6 steps\n",
    "forecast1 = arima_result.forecast(steps=6)\n",
    "forecast1.index = validationSet.index  # Align index\n",
    "\n",
    "# Compare with actuals\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': validationSet['SalesAmount'],\n",
    "    'Forecast': forecast1\n",
    "})\n",
    "\n",
    "print(comparison_df)\n",
    "\n",
    "\n",
    "'''\n",
    "Forecast table:\n",
    " Month\t  ActualSales\tForecast\t% Error\n",
    "\n",
    "Jul-2023\t10,042\t     9,404\t   6.36% under\n",
    "Aug-2023\t11,566\t     10,522\t   9.02% under\n",
    "Sep-2023\t11,759\t     11,087\t   5.7% under\n",
    "Oct-2023\t11,890\t     11,373\t   4.34% under\n",
    "Nov-2023\t11,770\t     11,517\t   2.15% under\n",
    "Dec-2023\t18,289\t     11,590\t   36.6% under \n",
    "\n",
    "The model is under-predicting sales, especially in December, which is a holiday month with higher sales.\n",
    "# This suggests the model may not fully capture seasonal spikes, especially during holidays.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "#### Calculate Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "mae = mean_absolute_error(comparison_df['Actual'], comparison_df['Forecast'])\n",
    "print(\"Mean Absolute Error (MAE) ARIMA 1,0,1: \", mae)\n",
    "\n",
    "mse = mean_squared_error(comparison_df['Actual'], comparison_df['Forecast'])\n",
    "print(f\"Mean Squared Error (MSE) ARIMA 1,0,1: {mse:.2f}\")\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE) ARIMA 1,0,1: {rmse:.2f}\")\n",
    "\n",
    "\n",
    "mape = mean_absolute_percentage_error(comparison_df['Actual'], comparison_df['Forecast'])\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) ARIMA 1,0,1: {mape:.2f}%\")\n",
    "\n",
    "\n",
    "'''\n",
    "MEAN ABSOLUTE ERROR (MAE)\n",
    "\n",
    "Date\t    Actual\tForecast\t Error\t  Absolute Error   \n",
    "2023-07-01\t10042\t9404.25\t    -637.75\t     637.75\n",
    "2023-08-01\t11566\t10522.26\t-1043.74\t 1043.74\n",
    "2023-09-01\t11759\t11087.36\t-671.64\t     671.64\n",
    "2023-10-01\t11890\t11372.99\t-517.01\t     517.01\n",
    "2023-11-01\t11770\t11517.35\t-252.65   \t 252.65\n",
    "2023-12-01\t18289\t11590.32\t-6698.68\t 6698.68\n",
    "\n",
    "MAE= 637.75 + 1043.74 + 671.64 + 517.01 + 252.65 + 6698.68 = 9821.47\n",
    "\n",
    "9821.47 / 6 = 1636.91\n",
    "\n",
    "\"ARIMA(1,0,1) model makes an average absolute forecasting error of ~1637 sales units over the validation period.\"\n",
    "\n",
    "\n",
    "MEAN SQUARED ERROR (MSE):\n",
    "\n",
    "Date\t    Actual\t  Forecast\t   (Actual - Forecast)²\n",
    "2023-07-01\t10042\t9404.249277\t    (637.750723)² =     406725.75\n",
    "2023-08-01\t11566\t10522.267519\t(1043.732481)² =    1089205.27\n",
    "2023-09-01\t11759\t11087.363305\t(671.636695)² =     451096.78\n",
    "2023-10-01\t11890\t11372.987668\t(517.012332)² =     267305.76\n",
    "2023-11-01\t11770\t11517.354833\t(252.645167)² =     63831.48\n",
    "2023-12-01\t18289\t11590.324373\t(6698.675627)² =    44881612.40\n",
    "\n",
    "MSE: Sum of squared errors: 406725.75 + 1089205.27 + 451096.78 + 267305.76 + 63831.48 + 44881612.40 = 47614777.44\n",
    "\n",
    "MSE = 47614777.44 / 6 = 7935796.24\n",
    "\"ARIMA(1,0,1) model has a mean squared error of ~7935796.24 sales units over the validation period.\"\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "#### Plot Forecast vs Actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(validationSet['SalesAmount'], label='Actual', marker='o')\n",
    "plt.plot(forecast1, label='Forecast', marker='x')\n",
    "plt.title('ARIMA Forecast vs Actuals')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales Amount')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Completely misses non-linear events like the December sales spike (e.g., holidays, promotions, end-of-year push\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### 3.3 Lets try again by implementing ARIMA (Autoregressive Integrated Moving Average) 1,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingSet = sales_data_with_lags.iloc[:-6] # First 30 months for training (Jan 2021-Jun 2023)\n",
    "# validationSet = sales_data_with_lags.iloc[-6:] # Last 6 months for validation (Jul 2023 - Dec 2023)\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "arima_model2 = ARIMA(trainingSet['SalesAmount'], order = (1,0,0))\n",
    "arima_result2 = arima_model2.fit()\n",
    "print(arima_result2.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast2 = arima_result2.forecast(steps = 6)\n",
    "forecast2.index = validationSet.index\n",
    "\n",
    "a = pd.DataFrame({\n",
    "    'Actual': validationSet['SalesAmount'],\n",
    "    'Forecast': forecast2\n",
    "})\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "#### Calculate Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "mae2 = mean_absolute_error(a['Actual'], a['Forecast'])\n",
    "mse2 = mean_squared_error(a['Actual'], a['Forecast'])\n",
    "RMSE2 = np.sqrt(mse2)   \n",
    "MAPE2 = mean_absolute_percentage_error(a['Actual'], a['Forecast'])\n",
    "\n",
    "print(f\"Mean Absolute Error (ARIMA(1,0,0)): {mae2:.2f}\")\n",
    "print(f\"Mean Squared Error (ARIMA(1,0,0)): {mse2:.2f}\") \n",
    "print(f\"Root Mean Squared Error (ARIMA(1,0,0)): {RMSE2:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (ARIMA(1,0,0)): {MAPE2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(validationSet['SalesAmount'], label = 'Actual', marker = 'o')\n",
    "plt.plot(forecast2, label = 'Forecast', marker = 'x')\n",
    "plt.title('ARIMA(1,0,0) Forecast vs Actuals')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales Amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### 3.4 Lets try again by implementing ARIMA (Autoregressive Integrated Moving Average) 0,0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "arima_model3 = ARIMA(trainingSet['SalesAmount'], order = (0,0,1))\n",
    "arima_result3 = arima_model3.fit()\n",
    "print(arima_result3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast3 = arima_result3.forecast(steps = 6)\n",
    "forecast3.index = validationSet.index\n",
    "\n",
    "b = pd.DataFrame({'Actual': validationSet['SalesAmount'], 'Forecast': forecast3})\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "#### Calculate Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "mae3 = mean_absolute_error(b['Actual'], b['Forecast'])\n",
    "mse3 = mean_squared_error(b['Actual'], b['Forecast'])\n",
    "RMSE3 = np.sqrt(mse3)\n",
    "MAPE3 = mean_absolute_percentage_error(b['Actual'], b['Forecast'])\n",
    "\n",
    "print(f\"Mean Absolute Error (ARIMA(0,0,1)): {mae3:.2f}\")\n",
    "print(f\"Mean Squared Error (ARIMA(0,0,1)): {mse3:.2f}\")\n",
    "print(f\"Root Mean Squared Error (ARIMA(0,0,1)): {RMSE3:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (ARIMA(0,0,1)): {MAPE3:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(validationSet['SalesAmount'], label = 'Actual', marker = 'o')\n",
    "plt.plot(forecast3, label = 'Forecast', marker = 'x')\n",
    "plt.title(\"ARIMA Forecast vs Actuals 0,0,1\")\n",
    "plt.xlabel (\"Date\")\n",
    "plt.ylabel('Sales Amount')\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "### 3.4 Lets try again by implementing ARIMA (Autoregressive Integrated Moving Average) 0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "arima_model4 = ARIMA(trainingSet[\"SalesAmount\"], order = (0,0,0))\n",
    "arima_result4 = arima_model4.fit()\n",
    "print(arima_result4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast4 = arima_result4.forecast(steps = 6)\n",
    "forecast4.index = validationSet.index\n",
    "c = pd.DataFrame({\"Actual\" : validationSet['SalesAmount'],\n",
    "                   'Forecast': forecast4})\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "#### Calculate Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "mae4 = mean_absolute_error(c['Actual'], c['Forecast'])\n",
    "mse4 = mean_squared_error(c['Actual'], c['Forecast'])\n",
    "RMSE4 = np.sqrt(mse4)\n",
    "MAPE4 = mean_absolute_percentage_error(c['Actual'], c['Forecast'])\n",
    "\n",
    "print(f\"Mean Absolute Error ARIMA(0,0,0): {mae4: .2f}\")\n",
    "print(f\"Mean Squared Error ARIMA(0,0,0): {mse4:.2f}\")\n",
    "print(f\"Root Mean Squared Error ARIMA(0,0,0): {RMSE4:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error ARIMA(0,0,0): {MAPE4:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(validationSet[\"SalesAmount\"], label = 'Actual', marker = 'o')\n",
    "plt.plot(forecast4, label = 'Forecast', marker = 'x')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales Amount\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "### 3.4 Lets try again by implementing ARIMA (Autoregressive Integrated Moving Average) 1,1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "result = adfuller(trainingSet['SalesAmount'])\n",
    "print(result[1])  # p-value should be < 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "arima_model5 = ARIMA(trainingSet['SalesAmount'], order = (1,1,1))\n",
    "arima_result5  = arima_model5.fit()\n",
    "print(arima_result5.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast5 = arima_result5.forecast(steps = 6)\n",
    "forecast5.index = validationSet.index\n",
    "d = pd.DataFrame({'Actual': validationSet['SalesAmount'], 'Forecast': forecast5})\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "#### Calculate Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "mae5= mean_absolute_error(d['Actual'], d['Forecast'])\n",
    "mse5= mean_squared_error(d['Actual'], d['Forecast'])\n",
    "RMSE5= np.sqrt(mse2)   \n",
    "MAPE5= mean_absolute_percentage_error(d['Actual'], d['Forecast'])\n",
    "\n",
    "print(f\"Mean Absolute Error (ARIMA(1,0,0)): {mae5:.2f}\")\n",
    "print(f\"Mean Squared Error (ARIMA(1,0,0)): {mse5:.2f}\")\n",
    "print(f\"Root Mean Squared Error (ARIMA(1,0,0)): {RMSE5:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (ARIMA(1,0,0)): {MAPE5:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(validationSet[\"SalesAmount\"], label = 'Actual', marker = 'o')\n",
    "plt.plot(forecast5, label = 'Forecast', marker = 'x')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales Amount\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "### 3.3 Implement SARIMA (Seasonal Autoregressive Integrated Moving Average) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "#### Check Trends through Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separates data into\n",
    "    # Trend:A long-term increase or decrease in the data over time\n",
    "    # Seasonality: A pattern that repeats at regular intervals (e.g., weekly, monthly, yearly).\n",
    "    # Stationarity: A property of a time series where statistical properties (like mean and variance) remain constant over time.\n",
    "        # tells you: “Can I rely on past data to predict the future, or is the game changing?”\n",
    "\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ensure 'salesAmount' is a float type\n",
    "\n",
    "trainingSet['SalesAmount'] = trainingSet['SalesAmount'].astype(float)\n",
    "\n",
    "decomp = seasonal_decompose(trainingSet['SalesAmount'], model  = 'additive', period = 12)\n",
    "\n",
    "# period=12 implies monthly seasonality (e.g., one full cycle per year if the data is monthly).\n",
    "\n",
    "decomp.plot()\n",
    "\n",
    "plt.suptitle(\"Time Series Decomposition of Sales Data of The Training Set - Before SARIMA\")\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "\n",
    "1. Observed (Top Panel: \"SalesAmount\")\n",
    "This is my original time series: raw monthly sales data (Jan 2021 – Jun 2023).\n",
    "It fluctuates — there's clear seasonality, a growing trend, and some noise.\n",
    "\n",
    "2. Trend\n",
    "This line shows the underlying direction of the data by smoothing out seasonal and random effects.\n",
    "We can observe a gentle upward movement in sales over time — likely due to the base sales increasing with time (+50 every month in your code).\n",
    "The trend gets cut off at the start and end (because moving averages need several points to compute).\n",
    "\n",
    "3. Seasonal\n",
    "Captures repeating patterns that occur every 12 months.\n",
    "In my case, the pattern is cyclical and symmetric, peaking around October–December and dipping around January–February — \n",
    "exactly as I intended with your custom sinusoidal seasonal components in the code.\n",
    "\n",
    "SARIMA can model this repeating seasonality directly with the seasonal component (S in SARIMA).\n",
    "\n",
    "4. Residual (or \"Noise\") - Residuals are the \"what we couldn’t explain\" part\n",
    "Residuals are the unexpected changes — small bumps and dips that don’t follow any regular pattern.\n",
    "What’s left after removing both trend and seasonality: ideally, random noise.\n",
    "The residuals are relatively small and mostly hover around zero, suggesting my data is well-explained by trend + seasonality.\n",
    "However, there are some spikes (e.g., near late 2022 or early 2023), which may hint at unusual events (like promotions or holidays\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "#### Check Seasonality through ACF and PACF Plots - Before SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACF helps decide how many past values (lags) to use → this is AR part.\n",
    "# ACF helps decide how many past errors to use → this is MA part\n",
    "\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#ACF Plot\n",
    "plot_acf(trainingSet[\"SalesAmount\"], lags = 24) #  We are comparing the SalesAmount column to itself at different time shifts (lags).\n",
    "# with lags=24, you're checking how well current sales correlate with sales from each of the past 24 months, one lag at a time.\n",
    "\n",
    "plt.title(\"ACF Plot for Sales Amount of Training Set - Before SARIMA\")\n",
    "plt.xlabel(\"Lags\")\n",
    "plt.ylabel(\"ACF\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#PACF Plot\n",
    "\n",
    "plot_pacf(trainingSet[\"SalesAmount\"], lags = 15)\n",
    "plt.title(\"PACF Plot for Sales Amount of Training Set - Before SARIMA\")\n",
    "plt.xlabel(\"Lags\")\n",
    "plt.ylabel(\"PACF\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# How the comparison happens in the plot: Each vertical line (spike) on the ACF plot represents the calculated autocorrelation coefficient for a specific lag.\n",
    "# The height of the line tells you the strength of the correlation, and its direction (above or below zero) tells you if it's positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "#### Check Stationarity through Dickey-Fuller Test - Before SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p > 0.05 → Non-stationary;  Apply transformations: differencing, log, seasonal adjustment\n",
    "# p <= 0.05 → Stationary; No transformations needed. Proceed with ARIMA, SARIMA, or other time series models\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "r = adfuller(trainingSet['SalesAmount'])\n",
    "\n",
    "print(\"ADF Statistic:\", r[0])\n",
    "print(\"p-value: \", r[1])\n",
    "print(\"Number of Lags used: \", r[2])\n",
    "print(\"Critical Values:\")\n",
    "\n",
    "for key, value in r[4].items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "'''\n",
    "The combination of the ACF, PACF plots, and the ADF test provides a clear picture of the SalesAmount training set:\n",
    "\n",
    "Trend: The slow decay in the ACF (even after considering seasonality) and the non-stationary result from the ADF test suggest an underlying trend.\n",
    "\n",
    "Seasonality: Both ACF and PACF clearly show strong annual seasonality (spikes at lag 12 and its multiples). The presence of a strong spike at lag 12 in the PACF indicates a direct seasonal autoregressive component.\n",
    "\n",
    "Non-Stationarity: The ADF test confirms that the series is non-stationary at a typical 5% significance level.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "#### Check Missing Values in the entire Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "#### Check Missing Values in the entire Sales Amount Column of Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet[\"SalesAmount\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "#### SARIMA (1,1,1)(1,1,1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sarima_model = SARIMAX(trainingSet['SalesAmount'], order = (1,1,1), seasonal_order = (1,1,1,12), enforce_stationarity=False, enforce_invertibility=False)\n",
    "sarima_result = sarima_model.fit()\n",
    "\n",
    "print(sarima_result.summary())\n",
    "\n",
    "# The SARIMA (1,1,1)(1,1,1,12) is technically running fine, but most coefficients are not statistically significant (p-values >> 0.05).\n",
    "# This means the model is not very confident that these particular lags (both trend and seasonality) are contributing meaningfully to improving the prediction.\n",
    "# Statistically, themodel is syntactically valid but weak in predictive power — possibly due to\n",
    "    # Small dataset (only 30 observations)\n",
    "    # Overfitting seasonal terms\n",
    "    # Lack of external (exogenous) features like promotions or holidays\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_sarima = sarima_result.forecast(steps = 6)\n",
    "\n",
    "forecast_sarima.index = validationSet.index\n",
    "\n",
    "comparison = pd.DataFrame({'Actual': validationSet['SalesAmount'], 'Forecast': forecast_sarima})\n",
    "print(comparison)\n",
    "\n",
    "comparison.plot(title=\"SARIMA Forecast vs Actual Sales\", figsize=(10,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "#### Calculate Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "mae_sarima = mean_absolute_error(comparison['Actual'], comparison['Forecast'])\n",
    "mse_sarima = mean_squared_error(comparison['Actual'], comparison['Forecast'])\n",
    "rmse_sarima = np.sqrt(mse_sarima)\n",
    "mape_sarima = mean_absolute_percentage_error(comparison['Actual'], comparison['Forecast'])\n",
    "\n",
    "print(f\"Mean Absolute Error for SARIMA (1,1,1)(1,1,1,12): , {mae_sarima:.2f}\")\n",
    "print(f\"Mean Squared Error for SARIMA (1,1,1)(1,1,1,12): , {mse_sarima:.2f}\")\n",
    "print(f\"Root Mean Squared Error for SARIMA (1,1,1)(1,1,1,12): , {rmse_sarima:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) SARIMA (1,1,1)(1,1,1,12): {mape_sarima:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "#### SARIMA (1,1,1)(1,1,1,12) with exogenous features (promotion and holiday month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_features = trainingSet[['Promotion', 'HolidayMonth']]\n",
    "model1 = SARIMAX(trainingSet['SalesAmount'], \n",
    "                exog=exog_features, \n",
    "                order=(1,1,1), \n",
    "                seasonal_order=(1,1,1,12),\n",
    "                enforce_stationarity=False, \n",
    "                enforce_invertibility=False)\n",
    "sarima_exog_result = model1.fit()\n",
    "print(sarima_exog_result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exogenous values for the forecast horizon (next 6 months)\n",
    "future_exog = validationSet[['Promotion', 'HolidayMonth']]\n",
    "\n",
    "# Forecast with future exogenous features\n",
    "forecast_sarima_exog = sarima_exog_result.forecast(steps=6, exog=future_exog)\n",
    "\n",
    "# Align forecast index\n",
    "forecast_sarima_exog.index = validationSet.index\n",
    "\n",
    "# Compare forecast with actual\n",
    "comparison1 = pd.DataFrame({\n",
    "    'Actual': validationSet['SalesAmount'],\n",
    "    'Forecast': forecast_sarima_exog\n",
    "})\n",
    "print(comparison1)\n",
    "\n",
    "# Plot\n",
    "comparison1.plot(title=\"SARIMA (1,1,1)(1,1,1,12) with Exogenous - Forecast vs Actual Sales\", figsize=(10,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "mae_sarima_exog = mean_absolute_error(comparison1['Actual'], comparison1['Forecast'])\n",
    "mse_sarima_exog = mean_squared_error(comparison1['Actual'], comparison1['Forecast'])\n",
    "rmse_sarima_exog = np.sqrt(mse_sarima_exog)\n",
    "mape_sarima_exog = mean_absolute_percentage_error(comparison1['Actual'], comparison1['Forecast'])\n",
    "\n",
    "print(f\"Mean Absolute Error for SARIMA exog (1,1,1)(1,1,1,12): {mae_sarima_exog:.2f}\")\n",
    "print(f\"Mean Squared Error for SARIMA exog (1,1,1)(1,1,1,12): , {mse_sarima_exog:.2f}\")\n",
    "print(f\"Root Mean Squared Error for SARIMA exog (1,1,1)(1,1,1,12): , {rmse_sarima_exog:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) SARIMA exog (1,1,1)(1,1,1,12): {mape_sarima_exog:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "### SARIMA (1,1,2)(1,0,1,12) with exogenous features (promotion and holiday month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_features2 = trainingSet[['Promotion', 'HolidayMonth']]\n",
    "model2 = SARIMAX(trainingSet['SalesAmount'], exog = exog_features2, order = (1,1,2),\n",
    "                  seasonal_order=(1,0,1,12), enforce_invertibility=False, enforce_stationarity=False) \n",
    "\n",
    "sarima_exog_result2 = model2.fit()\n",
    "print(sarima_exog_result2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_exog2 = validationSet[['Promotion', 'HolidayMonth']]\n",
    "\n",
    "forecast_sarima_exog2 = sarima_exog_result2.forecast(steps = 6, exog = future_exog2)\n",
    "\n",
    "forecast_sarima_exog2.index = validationSet.index\n",
    "\n",
    "\n",
    "comparison2 = pd.DataFrame({'Actual' : validationSet['SalesAmount'], 'Forecast': forecast_sarima_exog2})\n",
    "\n",
    "print(comparison2)\n",
    "\n",
    "comparison2.plot(title = \"SARIMA with Exogenous - Forecast vs Actual Sales - SARIMA (1,1,2)(1,0,1,12)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "mae_sarima_exog2 = mean_absolute_error(comparison2['Actual'], comparison2['Forecast'])\n",
    "mse_sarima_exog2 = mean_squared_error(comparison2['Actual'], comparison2['Forecast'])\n",
    "rmse_sarima_exog2 = np.sqrt(mse_sarima_exog2)\n",
    "mape_sarima_exog2 = mean_absolute_percentage_error(comparison2['Actual'], comparison2['Forecast'])\n",
    "\n",
    "print(f\"Mean Absolute Error for SARIMA exog (1,1,2)(1,0,1,12): {mae_sarima_exog2:.2f}\")\n",
    "print(f\"Mean Squared Error for SARIMA exog (1,1,2)(1,0,1,12): , {mse_sarima_exog2:.2f}\")\n",
    "print(f\"Root Mean Squared Error for SARIMA exog (1,1,2)(1,0,1,12): {rmse_sarima_exog2:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) SARIMA exog (1,1,2)(1,0,1,12): {mape_sarima_exog2:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "### SARIMA (2,1,2)(0,1,0,12) with exogenous features (promotion and holiday month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "exog_features3 = trainingSet[['Promotion', 'HolidayMonth']]\n",
    "\n",
    "model3 = SARIMAX(trainingSet['SalesAmount'], exog = exog_features, order = (2,1,2),\n",
    "                  seasonal_order=(0,1,0,12), enforce_invertibility=False, enforce_stationarity=False)\n",
    "\n",
    "\n",
    "sarima_exog_result3 = model3.fit()\n",
    "\n",
    "print(sarima_exog_result3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_exog3 = validationSet[['Promotion', 'HolidayMonth']]\n",
    "\n",
    "forecast_sarima_exog3 = sarima_exog_result3.forecast(steps = 6, exog = future_exog3)\n",
    "\n",
    "forecast_sarima_exog3.index = validationSet.index\n",
    "\n",
    "comparison3 = pd.DataFrame({'Actual': validationSet['SalesAmount'], 'Forecast' : forecast_sarima_exog3})\n",
    "\n",
    "print(comparison3)\n",
    "\n",
    "\n",
    "comparison3.plot(title = \"SARIMA (2,1,2)(0,1,0,12) with exogenous features (promotion and holiday month) \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "mae_sarima_exog3 = mean_absolute_error(comparison3['Actual'], comparison3['Forecast'])\n",
    "mse_sarima_exog3 = mean_squared_error(comparison3['Actual'], comparison3['Forecast'])\n",
    "rmse_sarima_exog3 = np.sqrt(mse_sarima_exog3)\n",
    "mape_sarima_exog3 = mean_absolute_percentage_error(comparison3['Actual'], comparison3['Forecast'])\n",
    "\n",
    "print(f\"Mean Absolute Error for SARIMA exog (2,1,2)(0,1,0,12): {mae_sarima_exog3:.2f}\")\n",
    "print(f\"Mean Squared Error for SARIMA exog (2,1,2)(0,1,0,12): , {mse_sarima_exog3:.2f}\")\n",
    "print(f\"Root Mean Squared Error for SARIMA exog (2,1,2)(0,1,0,12): , {rmse_sarima_exog3:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) for SARIMA exog (2,1,2)(0,1,0,12): {mape_sarima_exog3:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "### Prophet Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "##### Pre reqs for Prophet\n",
    "###### 1.\tProper Date-Time Formatting\n",
    "###### Your dataset must have a datetime column in standard format (YYYY-MM-DD) named \"ds\".\n",
    "######\tThe column representing the values to forecast must be named \"y\" (e.g., sales, revenue, demand).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data\n",
    "\n",
    "'''\n",
    "Observations\n",
    "Promotions Are Sparse: Only 6 promotional months out of 48 (12.5%), which is lower than the 20% expected from your generation code.\n",
    " This may weaken the statistical signal of the Promotion variable.\n",
    "\n",
    "Holiday Effect Appears Real: December sales are consistently higher:\n",
    "\n",
    "2020-12: 14,761\n",
    "\n",
    "2021-12: 13,966\n",
    "\n",
    "2022-12: 15,643\n",
    "\n",
    "2023-12: 18,289\n",
    "\n",
    "This supports holiday seasonality, even though your SARIMA models showed mixed results for HolidayMonth significance.\n",
    "\n",
    "Trend + Seasonality: There's a mild upward trend and some seasonal patterns (e.g., dips around summer, spikes around winter), aligning with your earlier synthetic generation logic.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "\n",
    "\n",
    "df_prophet = sales_data.reset_index()[['Date', 'SalesAmount', 'Promotion', 'HolidayMonth']].copy()\n",
    "\n",
    "# .copy() creates a new DataFrame that is a copy of the original, so any changes made to df_prophet won't affect sales_data.\n",
    "\n",
    "# Your dataset must have a datetime column in standard format (YYYY-MM-DD) named \"ds\".\n",
    "# Prophet doesn't require a DataFrame with datetime as the index. \n",
    "# It just needs a column named ds with dates, so we first flatten the DataFrame.\n",
    "\n",
    "df_prophet.rename(columns = {'Date':'ds', 'SalesAmount':'y'}, inplace = True)\n",
    "# inplace=True modifies the original DataFrame directly, renaming the columns to match Prophet's requirements:\n",
    "\n",
    "df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
    "# This line ensures that the \"ds\" column is in datetime format (YYYY-MM-DD) — essential for time-based modeling.\n",
    "\n",
    "# Define Holiday Calendar\n",
    "# Only December is a \"holiday month\" in my context\n",
    "# Defining a holiday calendar in Prophet is recommended when you want Prophet’s internal holiday modeling features.\n",
    "# A regressor with the same info can be used as a shortcut, but it does not fully replace the built-in holiday mechanism\n",
    "\n",
    "\n",
    "# Prophet’s holiday calendar is a built-in special mechanism that:\n",
    "# Treats holidays as categorical events with possible multiple days effects (via lower_window and upper_window).\n",
    "# enables Prophet’s built-in holiday effect machinery\n",
    "\n",
    "december_holidays = pd.DataFrame({\n",
    "    'holiday': 'december_bump',\n",
    "    'ds': df_prophet[df_prophet['ds'].dt.month == 12]['ds'],\n",
    "    'lower_window': 0,\n",
    "    'upper_window': 0\n",
    "})\n",
    "\n",
    "'''\n",
    "lower_window: Number of days before the holiday date to include in the holiday effect.\n",
    "upper_window: Number of days after the holiday date to include in the holiday effect.\n",
    "\n",
    "In my code, both are set to 0, meaning the holiday effect applies only on the exact date(s) in December, with no extension before or after.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Outlier Detection using z-score method\n",
    "# The z-score (also known as the standard score) measures how many standard deviations a data point is from the mean of a dataset.\n",
    "# It tells you whether a value is typical or unusual compared to the rest of the data\n",
    "# a z-score of:\n",
    "#       0 means the value is exactly at the mean.\n",
    "#      +1 means it’s 1 standard deviation above the mean.\n",
    "#      −2 means it’s 2 standard deviations below the mean.\n",
    "\n",
    "from scipy.stats import zscore\n",
    "df_prophet['z_score'] = zscore(df_prophet['y'])\n",
    "outliers = df_prophet[np.abs(df_prophet['z_score']) > 3] # np.abs() — Returns the absolute value of a number or array of numbers. The absolute value of a number is its distance from zero — without considering direction.\n",
    "# This line filters the DataFrame to find rows where the absolute z-score is greater than 3, indicating potential outliers.\n",
    "# A z-score greater than 3 or less than -3 is often considered an outlier in many statistical analyses.\n",
    "\n",
    "# print(\"Outliers detected based on z-score: \\n\", outliers[['ds', 'y', 'z_score']])\n",
    "print(\"Outliers detected based on z-score:\", len(outliers))\n",
    "\n",
    "# z-score column is not needed for Prophet, so we can drop it\n",
    "df_prophet.drop(columns = ['z_score'], inplace = True)\n",
    "\n",
    "prophet_model = Prophet(\n",
    "    yearly_seasonality = False,\n",
    "    weekly_seasonality = False,\n",
    "    daily_seasonality = False,\n",
    "    holidays = december_holidays,\n",
    "    seasonality_mode= 'additive').add_seasonality(name = 'yearly_custom', period = 12, fourier_order = 5)\n",
    "\n",
    "# This initializes a Prophet model with no built-in seasonality (yearly, weekly, daily) but adds a custom yearly seasonality with a 12-month period.\n",
    "# Because the data is monthly, we want to capture yearly patterns without the default daily/weekly noise.\n",
    "\n",
    "prophet_model.add_regressor('Promotion')\n",
    "prophet_model.add_regressor('HolidayMonth')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(december_holidays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "#### Train Prophet - 1st Attempt - Seasonality Mode: Additive - Fourier_order = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_prophet.iloc[:42].copy() # Selects the first 42 rows of df_prophet as the training set (likely Jan 2020 to June 2023).\n",
    "evaluate_df = df_prophet[['ds', 'Promotion', 'HolidayMonth']].iloc[42:].copy() # Selects the last 6 rows (row 42 to end), i.e., July to December 2023, as the evaluation (future) set.\n",
    "\n",
    "prophet_model.fit(train_df)\n",
    "\n",
    "future = evaluate_df.copy()\n",
    "\n",
    "forecast = prophet_model.predict(future)\n",
    "forecast = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "\n",
    "# Selects only the most relevant columns from the full forecast:\n",
    "    # ds: date\n",
    "    # yhat: predicted sales\n",
    "    # yhat_lower, yhat_upper: lower and upper bounds of the forecast interval (confidence range).\n",
    "\n",
    "comparison_prophet= pd.concat([\n",
    "    df_prophet[['ds', 'y']].iloc[42:].reset_index(drop=True),\n",
    "    forecast[['yhat']].round(2)\n",
    "], axis=1)\n",
    "\n",
    "comparison_prophet.columns = ['ds', 'Actual', 'Forecast']\n",
    "\n",
    "\n",
    "\n",
    "print(comparison_prophet)\n",
    "# Here’s what really happened (Actual), and here’s what my model thought would happen (Forecast).”\n",
    "# It's the final scoreboard for evaluating how well Prophet predicted reality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "mae_prophet = mean_absolute_error(comparison_prophet['Actual'], comparison_prophet['Forecast'])\n",
    "mse_prophet = mean_squared_error(comparison_prophet['Actual'], comparison_prophet['Forecast'])\n",
    "rmse_prophet = np.sqrt(mse_prophet)\n",
    "mape_prophet = mean_absolute_percentage_error(comparison_prophet['Actual'], comparison_prophet['Forecast'])\n",
    "\n",
    "print(f\"Mean Absolute Error for Prophet: {mae_prophet:.2f}\")\n",
    "print(f\"Mean Squared Error for Prophet: {mse_prophet:.2f}\")\n",
    "print(f\"Root Mean Squared Error for Prophet: {rmse_prophet:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) for Prophet: {mape_prophet:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_prophet.plot(title=\"Prophet Forecast vs Actual Sales\", x = 'ds', y = ,figsize=(10,5))\n",
    "comparison_prophet.plot(title=\"Prophet Forecast vs Actual Sales\", x='ds', y=['Actual', 'Forecast'], figsize=(10, 5), marker='o')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales Amount\")\n",
    "plt.legend([\"Actual Sales\", \"Forecasted Sales\"])\n",
    "plt.grid()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.title(\"Prophet Forecast vs Actual Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "#### Train Prophet - 2nd Attempt - Seasonality Mode: Additive - Fourier order = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "\n",
    "\n",
    "df_prophet2 = sales_data.reset_index()[['Date', 'SalesAmount', 'Promotion', 'HolidayMonth']].copy()\n",
    "\n",
    "df_prophet2.rename(columns = {'Date':'ds', 'SalesAmount':'y'}, inplace = True)\n",
    "\n",
    "df_prophet2['ds'] = pd.to_datetime(df_prophet2['ds'])\n",
    "\n",
    "december_holidays2 = pd.DataFrame({\n",
    "    'holiday': 'december_bump',\n",
    "    'ds': df_prophet2[df_prophet2['ds'].dt.month == 12]['ds'],\n",
    "    'lower_window': 0,\n",
    "    'upper_window': 0\n",
    "})\n",
    "\n",
    "from scipy.stats import zscore\n",
    "df_prophet2['z_score'] = zscore(df_prophet2['y'])\n",
    "outliers2 = df_prophet2[np.abs(df_prophet2['z_score']) > 3] \n",
    "print(\"Outliers detected based on z-score:\", len(outliers2))\n",
    "\n",
    "\n",
    "prophet_model2 = Prophet(\n",
    "    yearly_seasonality = False,\n",
    "    weekly_seasonality = False,\n",
    "    daily_seasonality = False,\n",
    "    holidays = december_holidays2,\n",
    "    seasonality_mode= 'additive').add_seasonality(name = 'yearly_custom', period = 12, fourier_order = 10)\n",
    "\n",
    "\n",
    "prophet_model2.add_regressor('Promotion')\n",
    "prophet_model2.add_regressor('HolidayMonth')\n",
    "\n",
    "\n",
    "train_df2 = df_prophet2.iloc[:42].copy() # Selects the first 42 rows of df_prophet as the training set (likely Jan 2020 to June 2023).\n",
    "evaluate_df2 = df_prophet2[['ds', 'Promotion', 'HolidayMonth']].iloc[42:].copy() # Selects the last 6 rows (row 42 to end), i.e., July to December 2023, as the evaluation (future) set.\n",
    "prophet_model2.fit(train_df2)\n",
    "\n",
    "future2 = evaluate_df2.copy()\n",
    "\n",
    "forecast2 = prophet_model2.predict(future)\n",
    "forecast2 = forecast2[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "\n",
    "\n",
    "comparison_prophet2= pd.concat([\n",
    "    df_prophet2[['ds', 'y']].iloc[42:].reset_index(drop=True),\n",
    "    forecast2[['yhat']].round(2)\n",
    "], axis=1)\n",
    "\n",
    "comparison_prophet2.columns = ['ds', 'Actual', 'Forecast']\n",
    "print(comparison_prophet2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "mae_prophet2 = mean_absolute_error(comparison_prophet2['Actual'], comparison_prophet2['Forecast'])\n",
    "mse_prophet2 = mean_squared_error(comparison_prophet2['Actual'], comparison_prophet2['Forecast'])\n",
    "rmse_prophet2 = np.sqrt(mse_prophet2)\n",
    "mape_prophet2 = mean_absolute_percentage_error(comparison_prophet2['Actual'], comparison_prophet2['Forecast'])\n",
    "\n",
    "print(f\"Mean Absolute Error for Prophet: {mae_prophet2:.2f}\")\n",
    "print(f\"Mean Squared Error for Prophet: {mse_prophet2:.2f}\")\n",
    "print(f\"Root Mean Squared Error for Prophet: {rmse_prophet2:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) for Prophet: {mape_prophet2:.2f}%\")\n",
    "\n",
    "\n",
    "# comparison_prophet.plot(title=\"Prophet Forecast vs Actual Sales\", x = 'ds', y = ,figsize=(10,5))\n",
    "comparison_prophet2.plot(title=\"Prophet Forecast vs Actual Sales\", x='ds', y=['Actual', 'Forecast'], figsize=(10, 5), marker='o')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales Amount\")\n",
    "plt.legend([\"Actual Sales\", \"Forecasted Sales\"])\n",
    "plt.grid()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.title(\"Prophet Forecast vs Actual Sales\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "#### Train Prophet - 3rd Attempt - Seasonality Mode: Multiplicative - Fourier order = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "\n",
    "\n",
    "df_prophet3 = sales_data.reset_index()[['Date', 'SalesAmount', 'Promotion', 'HolidayMonth']].copy()\n",
    "\n",
    "df_prophet3.rename(columns = {'Date':'ds', 'SalesAmount':'y'}, inplace = True)\n",
    "\n",
    "df_prophet3['ds'] = pd.to_datetime(df_prophet3['ds'])\n",
    "\n",
    "december_holidays3 = pd.DataFrame({\n",
    "    'holiday': 'december_bump',\n",
    "    'ds': df_prophet3[df_prophet3['ds'].dt.month == 12]['ds'],\n",
    "    'lower_window': 0,\n",
    "    'upper_window': 0\n",
    "})\n",
    "\n",
    "from scipy.stats import zscore\n",
    "df_prophet3['z_score'] = zscore(df_prophet3['y'])\n",
    "outliers3 = df_prophet3[np.abs(df_prophet3['z_score']) > 3] \n",
    "print(\"Outliers detected based on z-score:\", len(outliers3))\n",
    "\n",
    "\n",
    "prophet_model3 = Prophet(\n",
    "    yearly_seasonality = False,\n",
    "    weekly_seasonality = False,\n",
    "    daily_seasonality = False,\n",
    "    holidays = december_holidays3,\n",
    "    seasonality_mode= 'multiplicative').add_seasonality(name = 'yearly_custom', period = 12, fourier_order = 5)\n",
    "\n",
    "\n",
    "prophet_model3.add_regressor('Promotion')\n",
    "prophet_model3.add_regressor('HolidayMonth')\n",
    "\n",
    "\n",
    "train_df3 = df_prophet3.iloc[:42].copy() # Selects the first 42 rows of df_prophet as the training set (likely Jan 2020 to June 2023).\n",
    "evaluate_df3 = df_prophet3[['ds', 'Promotion', 'HolidayMonth']].iloc[42:].copy() # Selects the last 6 rows (row 42 to end), i.e., July to December 2023, as the evaluation (future) set.\n",
    "prophet_model3.fit(train_df3)\n",
    "\n",
    "future3 = evaluate_df3.copy()\n",
    "\n",
    "forecast3 = prophet_model3.predict(future)\n",
    "forecast3 = forecast3[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "\n",
    "\n",
    "comparison_prophet3= pd.concat([\n",
    "    df_prophet3[['ds', 'y']].iloc[42:].reset_index(drop=True),\n",
    "    forecast3[['yhat']].round(2)\n",
    "], axis=1)\n",
    "\n",
    "comparison_prophet3.columns = ['ds', 'Actual', 'Forecast']\n",
    "print(comparison_prophet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "mae_prophet3 = mean_absolute_error(comparison_prophet3['Actual'], comparison_prophet3['Forecast'])\n",
    "mse_prophet3 = mean_squared_error(comparison_prophet3['Actual'], comparison_prophet3['Forecast'])\n",
    "rmse_prophet3 = np.sqrt(mse_prophet3)\n",
    "mape_prophet3 = mean_absolute_percentage_error(comparison_prophet3['Actual'], comparison_prophet3['Forecast'])\n",
    "\n",
    "print(f\"Mean Absolute Error for Prophet: {mae_prophet3:.2f}\")\n",
    "print(f\"Mean Squared Error for Prophet: {mse_prophet3:.2f}\")\n",
    "print(f\"Root Mean Squared Error for Prophet: {rmse_prophet3:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) for Prophet: {mape_prophet3:.2f}%\")\n",
    "\n",
    "\n",
    "# comparison_prophet.plot(title=\"Prophet Forecast vs Actual Sales\", x = 'ds', y = ,figsize=(10,5))\n",
    "comparison_prophet3.plot(title=\"Prophet Forecast vs Actual Sales\", x='ds', y=['Actual', 'Forecast'], figsize=(10, 5), marker='o')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales Amount\")\n",
    "plt.legend([\"Actual Sales\", \"Forecasted Sales\"])\n",
    "plt.grid()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.title(\"Prophet Forecast vs Actual Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "#### Train Prophet - 4th Attempt - Seasonality Mode: Multiplicative - Fourier order = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "\n",
    "\n",
    "df_prophet4 = sales_data.reset_index()[['Date', 'SalesAmount', 'Promotion', 'HolidayMonth']].copy()\n",
    "\n",
    "df_prophet4.rename(columns = {'Date':'ds', 'SalesAmount':'y'}, inplace = True)\n",
    "\n",
    "df_prophet4['ds'] = pd.to_datetime(df_prophet4['ds'])\n",
    "\n",
    "december_holidays4 = pd.DataFrame({\n",
    "    'holiday': 'december_bump',\n",
    "    'ds': df_prophet4[df_prophet4['ds'].dt.month == 12]['ds'],\n",
    "    'lower_window': 0,\n",
    "    'upper_window': 0\n",
    "})\n",
    "\n",
    "from scipy.stats import zscore\n",
    "df_prophet4['z_score'] = zscore(df_prophet4['y'])\n",
    "outliers4 = df_prophet4[np.abs(df_prophet4['z_score']) > 3] \n",
    "print(\"Outliers detected based on z-score:\", len(outliers4))\n",
    "\n",
    "\n",
    "prophet_model4 = Prophet(\n",
    "    yearly_seasonality = False,\n",
    "    weekly_seasonality = False,\n",
    "    daily_seasonality = False,\n",
    "    holidays = december_holidays4,\n",
    "    seasonality_mode= 'multiplicative').add_seasonality(name = 'yearly_custom', period = 12, fourier_order = 10)\n",
    "\n",
    "\n",
    "prophet_model4.add_regressor('Promotion')\n",
    "prophet_model4.add_regressor('HolidayMonth')\n",
    "\n",
    "\n",
    "train_df4 = df_prophet4.iloc[:42].copy() # Selects the first 42 rows of df_prophet as the training set (likely Jan 2020 to June 2023).\n",
    "evaluate_df4 = df_prophet4[['ds', 'Promotion', 'HolidayMonth']].iloc[42:].copy() # Selects the last 6 rows (row 42 to end), i.e., July to December 2023, as the evaluation (future) set.\n",
    "prophet_model4.fit(train_df4)\n",
    "\n",
    "future4 = evaluate_df4.copy()\n",
    "\n",
    "forecast4 = prophet_model4.predict(future)\n",
    "forecast4 = forecast4[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "\n",
    "\n",
    "comparison_prophet4= pd.concat([\n",
    "    df_prophet4[['ds', 'y']].iloc[42:].reset_index(drop=True),\n",
    "    forecast4[['yhat']].round(2)\n",
    "], axis=1)\n",
    "\n",
    "comparison_prophet4.columns = ['ds', 'Actual', 'Forecast']\n",
    "print(comparison_prophet4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "mae_prophet4 = mean_absolute_error(comparison_prophet4['Actual'], comparison_prophet4['Forecast'])\n",
    "mse_prophet4 = mean_squared_error(comparison_prophet4['Actual'], comparison_prophet4['Forecast'])\n",
    "rmse_prophet4 = np.sqrt(mse_prophet4)\n",
    "mape_prophet4 = mean_absolute_percentage_error(comparison_prophet4['Actual'], comparison_prophet4['Forecast'])\n",
    "\n",
    "print(f\"Mean Absolute Error for Prophet: {mae_prophet4:.2f}\")\n",
    "print(f\"Mean Squared Error for Prophet: {mse_prophet4:.2f}\")\n",
    "print(f\"Root Mean Squared Error for Prophet: {rmse_prophet4:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) for Prophet: {mape_prophet4:.2f}%\")\n",
    "\n",
    "\n",
    "# comparison_prophet.plot(title=\"Prophet Forecast vs Actual Sales\", x = 'ds', y = ,figsize=(10,5))\n",
    "comparison_prophet4.plot(title=\"Prophet Forecast vs Actual Sales\", x='ds', y=['Actual', 'Forecast'], figsize=(10, 5), marker='o')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales Amount\")\n",
    "plt.legend([\"Actual Sales\", \"Forecasted Sales\"])\n",
    "plt.grid()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.title(\"Prophet Forecast vs Actual Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "#### Train Prophet - 4th Attempt - Stick with Additive + Fourier=5: It gave the best MAE, RMSE, and MAPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "#### Experimenting with change points - changepoint_prior_scale = VARYING (BY HANNAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "\n",
    "\n",
    "df_prophet5 = sales_data.reset_index()[['Date', 'SalesAmount', 'Promotion', 'HolidayMonth']].copy()\n",
    "\n",
    "df_prophet5.rename(columns = {'Date':'ds', 'SalesAmount':'y'}, inplace = True)\n",
    "\n",
    "df_prophet5['ds'] = pd.to_datetime(df_prophet5['ds'])\n",
    "\n",
    "december_holidays5 = pd.DataFrame({\n",
    "    'holiday': 'december_bump',\n",
    "    'ds': df_prophet5[df_prophet5['ds'].dt.month == 12]['ds'],\n",
    "    'lower_window': 0,\n",
    "    'upper_window': 0\n",
    "})\n",
    "\n",
    "from scipy.stats import zscore\n",
    "df_prophet5['z_score'] = zscore(df_prophet5['y'])\n",
    "outliers5 = df_prophet5[np.abs(df_prophet5['z_score']) > 3] \n",
    "print(\"Outliers detected based on z-score:\", len(outliers4))\n",
    "\n",
    "\n",
    "# Adding manual change points to the Prophet model\n",
    "# Change points are specific dates where the model can adjust its trend.\n",
    "manual_change_points = ['2020-10-01', '2020-11-01', '2020-12-01', '2021-10-01', '2021-11-01', '2021-12-01', '2022-10-01', '2022-11-01', '2022-12-01']\n",
    "\n",
    "prophet_model5 = Prophet(\n",
    "    changepoints = manual_change_points,\n",
    "    changepoint_prior_scale = 0.5,\n",
    "    yearly_seasonality = False,\n",
    "    weekly_seasonality = False,\n",
    "    daily_seasonality = False,\n",
    "    holidays = december_holidays4,\n",
    "    seasonality_mode= 'additive').add_seasonality(name = 'yearly_custom', period = 12, fourier_order = 5)\n",
    "\n",
    "\n",
    "prophet_model5.add_regressor('Promotion')\n",
    "prophet_model5.add_regressor('HolidayMonth')\n",
    "\n",
    "\n",
    "train_df5 = df_prophet5.iloc[:42].copy() # Selects the first 42 rows of df_prophet as the training set (likely Jan 2020 to June 2023).\n",
    "evaluate_df5 = df_prophet5[['ds', 'Promotion', 'HolidayMonth']].iloc[42:].copy() # Selects the last 6 rows (row 42 to end), i.e., July to December 2023, as the evaluation (future) set.\n",
    "prophet_model5.fit(train_df5)\n",
    "\n",
    "future5 = evaluate_df5.copy()\n",
    "\n",
    "forecast5 = prophet_model5.predict(future)\n",
    "forecast5 = forecast5[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "\n",
    "\n",
    "comparison_prophet5= pd.concat([\n",
    "    df_prophet5[['ds', 'y']].iloc[42:].reset_index(drop=True),\n",
    "    forecast5[['yhat']].round(2)\n",
    "], axis=1)\n",
    "\n",
    "comparison_prophet5.columns = ['ds', 'Actual', 'Forecast']\n",
    "print(comparison_prophet5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "mae_prophet5 = mean_absolute_error(comparison_prophet5['Actual'], comparison_prophet5['Forecast'])\n",
    "mse_prophet5 = mean_squared_error(comparison_prophet5['Actual'], comparison_prophet5['Forecast'])\n",
    "rmse_prophet5 = np.sqrt(mse_prophet5)\n",
    "mape_prophet5 = mean_absolute_percentage_error(comparison_prophet5['Actual'], comparison_prophet5['Forecast'])\n",
    "\n",
    "print(f\"Mean Absolute Error for Prophet: {mae_prophet5:.2f}\")\n",
    "print(f\"Mean Squared Error for Prophet: {mse_prophet5:.2f}\")\n",
    "print(f\"Root Mean Squared Error for Prophet: {rmse_prophet5:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) for Prophet: {mape_prophet5:.2f}%\")\n",
    "\n",
    "\n",
    "# comparison_prophet.plot(title=\"Prophet Forecast vs Actual Sales\", x = 'ds', y = ,figsize=(10,5))\n",
    "comparison_prophet5.plot(title=\"Prophet Forecast vs Actual Sales\", x='ds', y=['Actual', 'Forecast'], figsize=(10, 5), marker='o')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales Amount\")\n",
    "plt.legend([\"Actual Sales\", \"Forecasted Sales\"])\n",
    "plt.grid()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.title(\"Prophet Forecast vs Actual Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "# 4 - Selecting  SARIMA(2,1,2)(0,1,0,12) + exog  the best model and retrain it on the entire historical dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "####  SARIMA(2,1,2)(0,1,0,12) + exog  on the entire historical dataset i.e all 48 months of historical data (Jan 2020 – Dec 2023). \n",
    "#### use this fully trained model to forecast the next 6 months (Jan 2024 – Jun 2024), ideally including confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "exog_features_final_SARIMA = sales_data[['Promotion', 'HolidayMonth']]\n",
    "\n",
    "final_SARIMA_model = SARIMAX(sales_data['SalesAmount'], exog = exog_features_final_SARIMA, order = (2,1,2), seasonal_order = (0,1,0,12),\n",
    "                             enforce_invertibility = False, enforce_stationarity =False)\n",
    "\n",
    "final_SARIMA_result = final_SARIMA_model.fit()\n",
    "\n",
    "print(final_SARIMA_result.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_exog_dates= pd.date_range(start = '2024-01-01', periods = 6, freq = 'MS')\n",
    "\n",
    "future_exog_data = pd.DataFrame({\n",
    "    'Promotion': [0,0,1,0,1,0], # Example pattern: Promotions in Mar and May\n",
    "    'HolidayMonth': [1, 0, 0, 0, 0, 0] # Example: January is a holiday month\n",
    "}, index = future_exog_dates)\n",
    "\n",
    "forecast_final_SARIMA = final_SARIMA_result.get_forecast(steps=6, exog=future_exog_data)\n",
    "forecast_ci = forecast_final_SARIMA.conf_int()\n",
    "forecast_mean = forecast_final_SARIMA.predicted_mean\n",
    "\n",
    "comparison_final_SARIMA = pd.DataFrame({\n",
    "    'Forecast': forecast_mean,\n",
    "    'Lower CI': forecast_ci.iloc[:, 0],\n",
    "    'Upper CI': forecast_ci.iloc[:, 1]\n",
    "}, index=future_exog_dates)\n",
    "\n",
    "print(comparison_final_SARIMA)\n",
    "\n",
    "\n",
    "comparison_final_SARIMA['Forecast'].plot(title=\"Final SARIMA Forecast vs Actual Sales\", figsize=(10,5), marker='o')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales Amount\")\n",
    "plt.legend([\"Forecast\", \"Lower CI\", \"Upper CI\"])\n",
    "plt.grid()\n",
    "plt.fill_between(comparison_final_SARIMA.index,\n",
    "                 comparison_final_SARIMA['Lower CI'],\n",
    "                 comparison_final_SARIMA['Upper CI'],\n",
    "                 color='skyblue', alpha=0.3, label='95% Confidence Interval')\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {},
   "source": [
    "#### Visualize the forecasts against historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_mean\n",
    "forecast_ci\n",
    "\n",
    "plt.plot(sales_data.index, sales_data['SalesAmount'], label = \"Historical Sales\", marker = 'o')\n",
    "plt.plot(forecast_mean.index, forecast_mean.values, label = \"Forecasted Sales\", marker = 'o', color='orange')\n",
    "plt.fill_between(conf_int.index, \n",
    "                 conf_int.iloc[:, 0], \n",
    "                 conf_int.iloc[:, 1], \n",
    "                 color='skyblue', alpha=0.3, label='95% Confidence Interval')\n",
    "\n",
    "plt.axvline(pd.to_datetime('2023-12-31'),color = 'red', linestyle = \"--\", label = \"Forecast Start\")\n",
    "\n",
    "plt.title(\"Final SARIMA Forecast vs Historical Sales\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales Amount\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "##### The final sales forecast data (e.g., in a CSV file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final sales forecast data (e.g., in a CSV file) downloadable\n",
    "\n",
    "comparison_final_SARIMA.to_csv('final_sarima_forecast.csv') \n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
